{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import xlrd\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as tf\n",
    "from scipy.stats import chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from typing import Optional\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USTCYuJi\\AppData\\Local\\Temp\\ipykernel_22220\\2803483812.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  data=data.drop('death',1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "          Sp  Hrate  Breath  BloodOxy   Temp  Whitecellcounts  label  \\\n2      151.0   86.0    18.0      99.0  36.94             15.7    0.0   \n3       73.0   55.0    19.0     100.0  36.50             17.0    0.0   \n6      115.0   68.0    18.0      97.0  35.50              7.9    0.0   \n8      129.0   72.0    14.0      93.0  36.61              4.3    0.0   \n9       87.0  124.0    25.0      90.0  39.39             22.4    1.0   \n...      ...    ...     ...       ...    ...              ...    ...   \n53544   96.0   63.0    21.0      98.0  35.20             11.7    0.0   \n53546  133.0   73.0     8.0      99.0  36.06              6.1    0.0   \n53547  166.0   99.0    23.0      96.0  36.89             20.2    0.0   \n53549  156.0   88.0    15.0      96.0  36.83              9.5    0.0   \n53550  152.0   96.0    19.0     100.0  37.17             18.6    0.0   \n\n      Conscious        age  \n2             A  55.881486  \n3             A  46.275517  \n6             A  81.592179  \n8             A  77.917014  \n9             A  81.280232  \n...         ...        ...  \n53544         A  45.096905  \n53546         B  63.643884  \n53547         A  77.565907  \n53549         B  43.884161  \n53550         A  82.773493  \n\n[35207 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sp</th>\n      <th>Hrate</th>\n      <th>Breath</th>\n      <th>BloodOxy</th>\n      <th>Temp</th>\n      <th>Whitecellcounts</th>\n      <th>label</th>\n      <th>Conscious</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>151.0</td>\n      <td>86.0</td>\n      <td>18.0</td>\n      <td>99.0</td>\n      <td>36.94</td>\n      <td>15.7</td>\n      <td>0.0</td>\n      <td>A</td>\n      <td>55.881486</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>73.0</td>\n      <td>55.0</td>\n      <td>19.0</td>\n      <td>100.0</td>\n      <td>36.50</td>\n      <td>17.0</td>\n      <td>0.0</td>\n      <td>A</td>\n      <td>46.275517</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>115.0</td>\n      <td>68.0</td>\n      <td>18.0</td>\n      <td>97.0</td>\n      <td>35.50</td>\n      <td>7.9</td>\n      <td>0.0</td>\n      <td>A</td>\n      <td>81.592179</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>129.0</td>\n      <td>72.0</td>\n      <td>14.0</td>\n      <td>93.0</td>\n      <td>36.61</td>\n      <td>4.3</td>\n      <td>0.0</td>\n      <td>A</td>\n      <td>77.917014</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>87.0</td>\n      <td>124.0</td>\n      <td>25.0</td>\n      <td>90.0</td>\n      <td>39.39</td>\n      <td>22.4</td>\n      <td>1.0</td>\n      <td>A</td>\n      <td>81.280232</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53544</th>\n      <td>96.0</td>\n      <td>63.0</td>\n      <td>21.0</td>\n      <td>98.0</td>\n      <td>35.20</td>\n      <td>11.7</td>\n      <td>0.0</td>\n      <td>A</td>\n      <td>45.096905</td>\n    </tr>\n    <tr>\n      <th>53546</th>\n      <td>133.0</td>\n      <td>73.0</td>\n      <td>8.0</td>\n      <td>99.0</td>\n      <td>36.06</td>\n      <td>6.1</td>\n      <td>0.0</td>\n      <td>B</td>\n      <td>63.643884</td>\n    </tr>\n    <tr>\n      <th>53547</th>\n      <td>166.0</td>\n      <td>99.0</td>\n      <td>23.0</td>\n      <td>96.0</td>\n      <td>36.89</td>\n      <td>20.2</td>\n      <td>0.0</td>\n      <td>A</td>\n      <td>77.565907</td>\n    </tr>\n    <tr>\n      <th>53549</th>\n      <td>156.0</td>\n      <td>88.0</td>\n      <td>15.0</td>\n      <td>96.0</td>\n      <td>36.83</td>\n      <td>9.5</td>\n      <td>0.0</td>\n      <td>B</td>\n      <td>43.884161</td>\n    </tr>\n    <tr>\n      <th>53550</th>\n      <td>152.0</td>\n      <td>96.0</td>\n      <td>19.0</td>\n      <td>100.0</td>\n      <td>37.17</td>\n      <td>18.6</td>\n      <td>0.0</td>\n      <td>A</td>\n      <td>82.773493</td>\n    </tr>\n  </tbody>\n</table>\n<p>35207 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'D:\\\\PycharmProjects\\\\pre_icu\\\\sepsis_0614\\\\Rcode\\\\SHENGLI\\\\MIMIC.xlsx'\n",
    "data = pd.read_excel(data_path)\n",
    "data.loc[((data['label']==1)&(data['death'].notnull())),'label']=2\n",
    "data.loc[((data['Conscious']!='A')&(data['Conscious'].notnull())),'Conscious']='B'\n",
    "data.head()\n",
    "data=data.drop('death',1)\n",
    "data=data.dropna()\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#print(sum(data['label']==1),sum(data['label']==2))\n",
    "data.to_csv('D:\\\\PycharmProjects\\\\pre_icu\\\\sepsis_0614\\\\Rcode\\\\SHENGLI\\\\MIMIC_data.csv',encoding='gbk')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#神经网络随机种子设定\n",
    "def seed_torch(seed):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] =str(seed)#禁止hash随机化（unknown）\n",
    "\n",
    "    torch.manual_seed(seed)#CPU种子\n",
    "    torch.cuda.manual_seed(seed)#当前GPU种子\n",
    "    torch.cuda.manual_seed_all(seed)#所有GPU种子\n",
    "\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "    torch.backends.cudnn.deterministic=True#cuDNN是NVIDIA针对神经网络的加速库，牺牲少量精度换取运算效率\n",
    "\n",
    "seed_number=2333"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def NEWS_score(data):\n",
    "    NEWS=pd.DataFrame(index=data.index,columns=['NEWS_score','呼吸_score','血氧_score','收缩压_score','心率_score','意识_score','体温_score','sepsis','危险程度'],data=0)\n",
    "    #用for循环简单明了\n",
    "    for i in data.index:\n",
    "        if (data.loc[i,'呼吸']<=8)|(data.loc[i,'呼吸']>=25):\n",
    "            NEWS.loc[i,'呼吸_score']=3\n",
    "        if (data.loc[i,'呼吸']>=21)&(data.loc[i,'呼吸']<=24):\n",
    "            NEWS.loc[i,'呼吸_score']=2\n",
    "        if (data.loc[i,'呼吸']>=9)&(data.loc[i,'呼吸']<=11):\n",
    "            NEWS.loc[i,'呼吸_score']=1\n",
    "\n",
    "        if data.loc[i,'血氧']<=91:\n",
    "            NEWS.loc[i,'血氧_score']=3\n",
    "        if (data.loc[i,'血氧']>=92)&(data.loc[i,'血氧']<=93):\n",
    "            NEWS.loc[i,'血氧_score']=2\n",
    "        if (data.loc[i,'血氧']>=94)&(data.loc[i,'血氧']<=95):\n",
    "            NEWS.loc[i,'血氧_score']=1\n",
    "\n",
    "        if (data.loc[i,'收缩压']<=90)|(data.loc[i,'收缩压']>=220):\n",
    "            NEWS.loc[i,'收缩压_score']=3\n",
    "        if (data.loc[i,'收缩压']>=91)&(data.loc[i,'收缩压']<=100):\n",
    "            NEWS.loc[i,'收缩压_score']=2\n",
    "        if (data.loc[i,'收缩压']>=101)&(data.loc[i,'收缩压']<=110):\n",
    "            NEWS.loc[i,'收缩压_score']=1\n",
    "\n",
    "        if (data.loc[i,'心率']<=40)|(data.loc[i,'心率']>=131):\n",
    "            NEWS.loc[i,'心率_score']=3\n",
    "        if (data.loc[i,'心率']>=111)&(data.loc[i,'心率']<=130):\n",
    "            NEWS.loc[i,'心率_score']=2\n",
    "        if ((data.loc[i,'心率']>=41)&(data.loc[i,'心率']<=50))|((data.loc[i,'心率']>=91)&(data.loc[i,'心率']<=110)):\n",
    "            NEWS.loc[i,'心率_score']=1\n",
    "\n",
    "        if data.loc[i,'意识']!=1:\n",
    "            NEWS.loc[i,'意识_score']=1\n",
    "\n",
    "        if data.loc[i,'体温']<=35:\n",
    "            NEWS.loc[i,'体温_score']=3\n",
    "        if data.loc[i,'体温']>=39.1:\n",
    "            NEWS.loc[i,'体温_score']=2\n",
    "        if ((data.loc[i,'体温']>=35.1)&(data.loc[i,'体温']<=36))|((data.loc[i,'体温']>=38.1)&(data.loc[i,'体温']<=39)):\n",
    "            NEWS.loc[i,'体温_score']=1\n",
    "\n",
    "    NEWS['NEWS_score']=NEWS.apply(lambda x:x.sum(),axis=1)\n",
    "    if 'sepsis' in data.columns:\n",
    "        NEWS['sepsis']=data.loc[:,'sepsis']\n",
    "    if '危险程度' in data.columns:\n",
    "        NEWS['危险程度']=data.loc[:,'危险程度']\n",
    "    return NEWS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ordinal Regression with Knowledge Distillation\n",
    "\n",
    "## 0 序言\n",
    "\n",
    "在一切开始之前，我们不得不介绍一下Ordinal Regression的背景和发展，这一读书结果我们将在读书笔记的PDF文件中展示。\n",
    "\n",
    "## 1 数据载入与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_path = 'wkp_lqy_220508-6.CSV'\n",
    "da = pd.read_csv(data_path,encoding='gbk')\n",
    "choose_feature=['意识','体温','心率','呼吸','收缩压','血氧','白细胞计数']\n",
    "choose_feature_c=['体温','心率','呼吸','收缩压','血氧','白细胞计数']\n",
    "data=da.loc[:,choose_feature]\n",
    "data['label']=da.loc[:,'sepsis']\n",
    "data.loc[(da['sepsis']==1)&(da['危险程度']==1),'label']=2\n",
    "data.head()\n",
    "#print(sum(data['label']==1),sum(data['label']==2))\n",
    "data.to_csv('C:\\\\Users\\\\USTCYuJi\\\\Desktop\\\\report-PPT\\\\221101\\\\data.csv',encoding='gbk')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "     意识    体温   心率  呼吸  收缩压  血氧  白细胞计数  label\n3480  0  36.3  100  20   75  96  22.92      0\n6440  0  36.4   93  20  162  97  11.14      0\n4634  0  36.4   70  20  148  98   6.24      0\n8005  0  36.3   87  28  126  98   2.94      0\n9187  1  39.9  110  20  177  98  10.66      0\n...  ..   ...  ...  ..  ...  ..    ...    ...\n492   1  36.6   67   0  104  97  18.81      2\n225   0  36.7   88  20  111  89  13.94      2\n1101  0  37.2  112  21  114  88   1.55      2\n527   0  36.0  144  22   74  91  30.40      2\n1110  0  36.3  105  32  120  74   6.64      2\n\n[7559 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>意识</th>\n      <th>体温</th>\n      <th>心率</th>\n      <th>呼吸</th>\n      <th>收缩压</th>\n      <th>血氧</th>\n      <th>白细胞计数</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3480</th>\n      <td>0</td>\n      <td>36.3</td>\n      <td>100</td>\n      <td>20</td>\n      <td>75</td>\n      <td>96</td>\n      <td>22.92</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6440</th>\n      <td>0</td>\n      <td>36.4</td>\n      <td>93</td>\n      <td>20</td>\n      <td>162</td>\n      <td>97</td>\n      <td>11.14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4634</th>\n      <td>0</td>\n      <td>36.4</td>\n      <td>70</td>\n      <td>20</td>\n      <td>148</td>\n      <td>98</td>\n      <td>6.24</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8005</th>\n      <td>0</td>\n      <td>36.3</td>\n      <td>87</td>\n      <td>28</td>\n      <td>126</td>\n      <td>98</td>\n      <td>2.94</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9187</th>\n      <td>1</td>\n      <td>39.9</td>\n      <td>110</td>\n      <td>20</td>\n      <td>177</td>\n      <td>98</td>\n      <td>10.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>492</th>\n      <td>1</td>\n      <td>36.6</td>\n      <td>67</td>\n      <td>0</td>\n      <td>104</td>\n      <td>97</td>\n      <td>18.81</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>225</th>\n      <td>0</td>\n      <td>36.7</td>\n      <td>88</td>\n      <td>20</td>\n      <td>111</td>\n      <td>89</td>\n      <td>13.94</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1101</th>\n      <td>0</td>\n      <td>37.2</td>\n      <td>112</td>\n      <td>21</td>\n      <td>114</td>\n      <td>88</td>\n      <td>1.55</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>527</th>\n      <td>0</td>\n      <td>36.0</td>\n      <td>144</td>\n      <td>22</td>\n      <td>74</td>\n      <td>91</td>\n      <td>30.40</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1110</th>\n      <td>0</td>\n      <td>36.3</td>\n      <td>105</td>\n      <td>32</td>\n      <td>120</td>\n      <td>74</td>\n      <td>6.64</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>7559 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预处理：将意识A转为0，意识B&C转为1\n",
    "data.loc[data['意识']=='A','意识']=0\n",
    "data.loc[(data['意识']=='B')|(data['意识']=='C'),'意识']=1\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "seed_torch(seed_number)\n",
    "tfd_train = {\n",
    "    0: 0.6,\n",
    "    1: 0.6,\n",
    "    2: 0.6\n",
    "}\n",
    "\n",
    "tfd_valid ={\n",
    "    0: 0.5,\n",
    "    1: 0.5,\n",
    "    2: 0.5\n",
    "}\n",
    "def tsampling(group, tfd):\n",
    "    name = group.name\n",
    "    frac = tfd[name]\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "train = data.groupby('label',group_keys=False).apply(tsampling, tfd_train)\n",
    "d_da0 = pd.concat([data, train,train]).drop_duplicates(keep= False)\n",
    "valid = d_da0.groupby('label',group_keys=False).apply(tsampling, tfd_valid)\n",
    "test = pd.concat([d_da0, valid,valid]).drop_duplicates(keep= False)\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 基本原理与算法实现\n",
    "\n",
    "### 2.1 Ordinal Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def _reduction(loss: torch.Tensor, reduction: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reduce loss\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss : torch.Tensor, [batch_size, num_classes]\n",
    "        Batch losses.\n",
    "    reduction : str\n",
    "        Method for reducing the loss. Options include 'elementwise_mean',\n",
    "        'none', and 'sum'.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : torch.Tensor\n",
    "        Reduced loss.\n",
    "    \"\"\"\n",
    "    if reduction == 'elementwise_mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'none':\n",
    "        return loss\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        raise ValueError(f'{reduction} is not a valid reduction')\n",
    "\n",
    "\n",
    "def cumulative_link_loss(y_pred: torch.Tensor, y_true: torch.Tensor,\n",
    "                         reduction: str = 'elementwise_mean',\n",
    "                         class_weights: Optional[np.ndarray] = None\n",
    "                         ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the negative log likelihood using the logistic cumulative link\n",
    "    function.\n",
    "    See \"On the consistency of ordinal regression methods\", Pedregosa et. al.\n",
    "    for more details. While this paper is not the first to introduce this, it\n",
    "    is the only one that I could find that was easily readable outside of\n",
    "    paywalls.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : torch.Tensor, [batch_size, num_classes]\n",
    "        Predicted target class probabilities. float dtype.\n",
    "    y_true : torch.Tensor, [batch_size, 1]\n",
    "        True target classes. long dtype.\n",
    "    reduction : str\n",
    "        Method for reducing the loss. Options include 'elementwise_mean',\n",
    "        'none', and 'sum'.\n",
    "    class_weights : np.ndarray, [num_classes] optional (default=None)\n",
    "        An array of weights for each class. If included, then for each sample,\n",
    "        look up the true class and multiply that sample's loss by the weight in\n",
    "        this array.\n",
    "    Returns\n",
    "    -------\n",
    "    loss: torch.Tensor\n",
    "    \"\"\"\n",
    "    eps = 1e-15\n",
    "    # torch.clamp用于限制上下界，torch.gather中1表示对列做，取的index是y_true\n",
    "    likelihoods = torch.clamp(torch.gather(y_pred, 1, y_true), eps, 1 - eps)\n",
    "    neg_log_likelihood = -torch.log(likelihoods)\n",
    "\n",
    "    if class_weights is not None:\n",
    "        # Make sure it's on the same device as neg_log_likelihood\n",
    "        class_weights = torch.as_tensor(class_weights,\n",
    "                                        dtype=neg_log_likelihood.dtype,\n",
    "                                        device=neg_log_likelihood.device)\n",
    "        neg_log_likelihood *= class_weights[y_true]\n",
    "\n",
    "    loss = _reduction(neg_log_likelihood, reduction)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class CumulativeLinkLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Module form of cumulative_link_loss() loss function\n",
    "    Parameters\n",
    "    ----------\n",
    "    reduction : str\n",
    "        Method for reducing the loss. Options include 'elementwise_mean',\n",
    "        'none', and 'sum'.\n",
    "    class_weights : np.ndarray, [num_classes] optional (default=None)\n",
    "        An array of weights for each class. If included, then for each sample,\n",
    "        look up the true class and multiply that sample's loss by the weight in\n",
    "        this array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction: str = 'elementwise_mean',\n",
    "                 class_weights: Optional[torch.Tensor] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor,\n",
    "                y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return cumulative_link_loss(y_pred, y_true,\n",
    "                                    reduction=self.reduction,\n",
    "                                    class_weights=self.class_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.0463e-45, 1.0000e+00, 6.8837e-42],\n",
      "        [8.0106e-52, 1.0000e+00, 6.2242e-39],\n",
      "        [1.0513e-43, 1.0000e+00, 1.9327e-31],\n",
      "        ...,\n",
      "        [3.1740e-52, 1.0000e+00, 1.5631e-42],\n",
      "        [2.0526e-57, 1.0000e+00, 1.0135e-55],\n",
      "        [7.8681e-50, 1.0000e+00, 4.9958e-42]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USTCYuJi\\AppData\\Local\\Temp\\ipykernel_36308\\2138851499.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pre_y=nn.functional.softmax(pre_y)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(pre_y)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m#logit_pre_y=lpre_y.exp()/(1+lpre_y.exp())#转到(0,1)之间\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m#f_0=logit_pre_y[y_train==0]\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m#f_0=torch.cat([1-f_0,f_0],dim=-1)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#qg_1=torch.tensor([1-new_g1.values,new_g1.values]).t()\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m#这两个loss的输入格式不一样，需要注意\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m loss \u001B[38;5;241m=\u001B[39m\u001B[43mloss_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpre_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m#+LAMBDA0*torch.nn.functional.kl_div(input=f_0.log(),target=qg_0,reduction='batchmean')+LAMBDA1*torch.nn.functional.kl_div(input=f_1.log(),target=qg_1,reduction='batchmean')\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m#epoch次数太多之后，f_0会非常接近0&1，这样在kl_div的计算中，f_0.log()会报错，此外，为了避免这种情况，还要对数据做标准化处理(总之数据标准化之后，就没出现数值错误)\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m#optimizer.zero_grad()\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m#loss.backward()\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m#optimizer.step()\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mCumulativeLinkLoss.forward\u001B[1;34m(self, y_pred, y_true)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, y_pred: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m     90\u001B[0m             y_true: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m---> 91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcumulative_link_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mclass_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weights\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mcumulative_link_loss\u001B[1;34m(y_pred, y_true, reduction, class_weights)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;124;03mCalculates the negative log likelihood using the logistic cumulative link\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03mfunction.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03mloss: torch.Tensor\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     54\u001B[0m eps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-15\u001B[39m\n\u001B[1;32m---> 55\u001B[0m likelihoods \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mclamp(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m)\u001B[49m, eps, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m eps)\n\u001B[0;32m     56\u001B[0m neg_log_likelihood \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlog(likelihoods)\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m class_weights \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;66;03m# Make sure it's on the same device as neg_log_likelihood\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "fc_net=nn.Sequential(\n",
    "        nn.Linear(len(choose_feature),3)\n",
    "    ).double()\n",
    "loss_func=CumulativeLinkLoss()\n",
    "optimizer = torch.optim.Adam(fc_net.parameters(), lr=0.01)\n",
    "epoch=100\n",
    "x_train=torch.tensor(train.loc[:,choose_feature].values.astype(\"float\"))\n",
    "y_train=torch.tensor(train.loc[:,'label'].values.astype(\"int64\"))\n",
    "x_valid=torch.tensor(valid.loc[:,choose_feature].values.astype(\"float\"))\n",
    "y_valid=torch.tensor(valid.loc[:,'label'].values.astype(\"int64\"))\n",
    "x_test=torch.tensor(test.loc[:,choose_feature].values.astype(\"float\"))\n",
    "y_test=torch.tensor(test.loc[:,'label'].values.astype(\"int64\"))\n",
    "for i in range(3):\n",
    "    logit_y = fc_net(x_train)\n",
    "    pre_y=nn.functional.softmax(pre_y)\n",
    "    print(pre_y)\n",
    "    #logit_pre_y=lpre_y.exp()/(1+lpre_y.exp())#转到(0,1)之间\n",
    "    #f_0=logit_pre_y[y_train==0]\n",
    "    #f_0=torch.cat([1-f_0,f_0],dim=-1)\n",
    "    #f_1=logit_pre_y[y_train==1]\n",
    "    #f_1=torch.cat([1-f_1,f_1],dim=-1)\n",
    "    #qg_0=torch.tensor([1-new_g0.values,new_g0.values]).t()\n",
    "    #qg_1=torch.tensor([1-new_g1.values,new_g1.values]).t()\n",
    "    #这两个loss的输入格式不一样，需要注意\n",
    "    loss =loss_func(pre_y,y_train)#+LAMBDA0*torch.nn.functional.kl_div(input=f_0.log(),target=qg_0,reduction='batchmean')+LAMBDA1*torch.nn.functional.kl_div(input=f_1.log(),target=qg_1,reduction='batchmean')\n",
    "    #epoch次数太多之后，f_0会非常接近0&1，这样在kl_div的计算中，f_0.log()会报错，此外，为了避免这种情况，还要对数据做标准化处理(总之数据标准化之后，就没出现数值错误)\n",
    "    #optimizer.zero_grad()\n",
    "    #loss.backward()\n",
    "    #optimizer.step()\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 结果展示与讨论\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-187ae89a",
   "language": "python",
   "display_name": "PyCharm (pycharm file)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}